---
title: "Logistic Regression on speed dating data"
author: "Jos√© Benardi de Souza Nunes"
date: 05/08/2018
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---

<br>

> This report contains regression models created based on data describing 5000 speed dates of 4 minutes of duration involving 310 american young adults. The original data were collected by Columbia Business professors. Further information and the data itself can be found in this [report repository](https://github.com/Benardi/speed-dating_analysis).

<br>

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(ggfortify)
library(gridExtra)
library(broom)
library(pscl)
library(ROCR)
library(vcd)
library(here)

theme_set(theme_bw())
```

<br>

***

<br>

# Data Overview

<br>

## The variables
<br>

```
The response variable is the variable that you are interested in learn something about.

A predictor variable is a variable used in regression to predict another variable.

Our response variable will be "dec", we want to study how well the predictor variables can help predict its behavior and how they impact it.
```

<br>

##### Each speed date had two participants, **p1** (participant 1) and **p2** (participant 2). For each speed date we the following variable were collected:

<br>

- **iid** : id of the participant p1 in the date
- **gender** : gender of p1, 0 = woman
- **order** : of the several dates in the night, this was the nth, according to this variable
- **pid** : id of participant p2
- **int_corr** : correlation between the interests of p1 and p2
- **samerace** : Are p1 and p2 of the same race?
- **age_o** : Age of p2
- **age** : Age of p1
- **field** : field of study of p1
- **race** : race of p1. The code is Black/African American=1; European/Caucasian-American=2; Latino/Hispanic American=3; Asian/Pacific Islander/Asian-American=4; Native American=5; Other=6
- **from** : from where p1 comes from
- **career** : what career p1 wants to follow sports, tvsports, exercise, dining, museums, art, hiking, gaming, clubbing, reading, tv, theater, movies, concerts, music, shopping, yoga : From 1 to 10, how interested p1 is in each one of these activities$
- **attr** : how attractive p1 thinks p2 is
- **sinc** : how sincere p1 thinks p2 is
- **intel** : how smart p1 thinks p2 is
- **fun** : how fun p1 thinks p2 is
- **amb** : how ambitious p1 thinks p2 is
- **shar** : how much p1 believes they both (p1 and p2) share the same interests and hobbies
- **like** : in general, how much does p1 likes p2?
- **prob** : how probable p1 thinks it's that p2 will want to meet again with p- (scale 1-10)
- **attr3_s** : how attractive p1 believes itself
- **sinc3_s** : how sincere p1 believes itself
- **intel3_s** : how smart p1 believes itself
- **fun3_s** : how fun p1 believes itself
- **amb3_s** : how ambitious p1 believes itself
- **dec** : whether p1 wants to meet p2 again given how the speed date went.

<br>

```{r}
data <- read_csv(here::here("data/speed-dating2.csv"),
                 col_types = cols(
                          .default = col_integer(),
                          int_corr = col_double(),
                          field = col_character(),
                          from = col_character(),
                          career = col_character(),
                          attr = col_double(),
                          sinc = col_double(),
                          intel = col_double(),
                          fun = col_double(),
                          amb = col_double(),
                          shar = col_double(),
                          like = col_double(),
                          prob = col_double(),
                          match_es = col_double(),
                          attr3_s = col_double(),
                          sinc3_s = col_double(),
                          intel3_s = col_double(),
                          fun3_s = col_double(),
                          amb3_s = col_double(),
                          dec = col_character()
                        )) %>% 
  mutate(dec = factor(dec),
         gender = factor(gender),
         samerace = factor(samerace),
         race = factor(race))

data %>%
  glimpse()
```

## Data exploration

```{r}
data %>%
  na.omit(race) %>%
  ggplot(aes(race,y=(..count..)/sum(..count..))) +
    geom_bar(color = "black",
           fill = "grey") +
  labs(x= "Participant Race",
       y = "Relative Frequency")
```

* Most of the participants are white (code = 2)
* There were no Native Americans involved (code = 5) 

```{r}
data %>%
  na.omit(intel) %>%
  ggplot(aes(intel, ..ndensity..)) +
  geom_histogram(color = "black",
                 fill = "grey",
                 breaks=seq(0, 10, by = 1)) +
  scale_x_continuous(breaks=seq(0, 10, by = 1)) +
  labs(x= "Intelligence (intel)",
       y = "Relative Density")
```

* Most of the time P1 gave P2 a score around 7 and 8 for intelligence.

```{r}
data %>%
  na.omit(attr) %>%
  ggplot(aes(attr, ..ndensity..)) +
  geom_histogram(color = "black",
                 fill = "grey",
                 breaks=seq(0, 10, by = 1)) +
  scale_x_continuous(breaks=seq(0, 10, by = 1)) +
  labs(x= "Attractivnes (attr)",
       y = "Relative Density")
```

* Most of the time P1 gave P2 a score around 5 and 6 for attractiveness.

```{r}
data %>%
  na.omit(amb) %>%
  ggplot(aes(amb, ..ndensity..)) +
  geom_histogram(color = "black",
                 fill = "grey",
                 breaks=seq(0, 10, by = 1)) +
  scale_x_continuous(breaks=seq(0, 10, by = 1)) +
  labs(x= "Ambition (amb)",
       y = "Relative Density")
```

* Most of the time P1 gave P2 a score around 6 and 7 for ambition.

```{r}
data %>%
  na.omit(sinc) %>%
  ggplot(aes(sinc, ..ndensity..)) +
  geom_histogram(color = "black",
                 fill = "grey",
                 breaks=seq(0, 10, by = 1)) +
  scale_x_continuous(breaks=seq(0, 10, by = 1)) +
  labs(x= "Sincerity (sinc)",
       y = "Relative Density")
```

* Most of the time P1 gave P2 a score around 6 and 8 for sincerity.

```{r}
data %>%
  na.omit(like) %>%
  ggplot(aes(like, ..ndensity..)) +
  geom_histogram(color = "black",
                 fill = "grey",
                 breaks=seq(0, 10, by = 1)) +
  scale_x_continuous(breaks=seq(0, 10, by = 1)) +
  labs(x= "Like (like)",
       y = "Relative Density")
```

* Most of the time P1 gave P2 a score around 5 and 6 for like.

## Splitting Data for Cross Validation

```{r}
data %>% # Keep only candidate predictor variables and response variable
  select(dec,fun, prob, order, amb,
         attr, sinc, prob, shar, 
         intel, like, gender, samerace) %>%
  na.omit() %>% # remove NAs
   mutate_at(
     .vars = vars(fun, prob, order,attr, ## Put numeric predictor variables on same scale 
                  sinc, like, prob, shar,intel),
     .funs = funs(as.numeric(scale(.)))) -> data_scaled

data_scaled %>%
  glimpse()
```

* Selecting promising predictors, filtering invalid numbers and putting the variables on appropriate scale

<br>

#### For the sake of simplicity we'll follow the (80/20) thumb rule (based on Pareto's principle) and put 80% of our dataset in the training set and 20% in the test set. 

<br>

```{r}
set.seed(101) # We set the set for reason of reproducibility

## Adding surrogate key to dataframe
data_scaled$id <- 1:nrow(data_scaled)

data_scaled %>% 
  dplyr::sample_frac(.8) -> training_data

training_data %>% 
  glimpse()
```

* Randomly selecting the **training data**


```{r}
dplyr::anti_join(data_scaled, 
                 training_data, 
                 by = 'id') -> testing_data
testing_data %>% 
  glimpse()
```

* The rest of the data will be the **testing data** (Disjoint sets)

<br>

***

<br>


# Explanation on logistic regression

<br>

> If you already know your way around logistic regression feel free to skip this section. If you ever feel unsure about it feel free to come back here and consult it.

<br>

## Formulas and definitions

<br>

Let's use as example our aforementioned response variable $dec$, and let's suppose we'll use as our sole predictor $like$. In the end it all boils down to a  conditional probability for a certain value of $dec$ and $like$:

<br>

$$\large P(y \ | \ x ) = z, \normalsize where: \ y = dec;\ x=like;$$

<br>

As you may have noticed $dec$ is a binary variable (p1 either says **yes** or **no**), for this reason we work with a sigmoid function for the probability of a certain outcome of $dec$:  

<br>

$$\large P(y\ | \ x)=\frac{e^{b_{0}+b_{1} \cdot x}}{1 + e^{b_{0}+b_{1} \cdot x}}, \; \normalsize where: \ y = dec;\ x=like;$$

<br>

However, to talk about how the predictor $like$ impacts the response variable $dec$, which means talking about $b_{1}$ it's more convenient to talk in terms of _**odds ratio**_:

<br>

$$\large \frac{P(y\ | \ x)}{1 - P(y \ | \ x)} =e^{b_{0}+b_{1} \cdot x_{1}}, \; \normalsize where: \ y = dec;\ x_{1}=like;$$

<br>

The libraries usually render the coefficients in the following form:

<br>

$$\large log(\frac{P(y\ | \ x)}{1 - P(y\ | \ x)}) =b_{0}+b_{1} \cdot x_{1}, \; \normalsize where: \ y = dec;\ x_{1}=like;$$


<br>

## Numeric example

<br>

In my experience an example with actual numbers helps a lot, so let's assume these are the following values for our variables (by means of the logistic regression):

 $b1 = 0.9441278, \\ b0 = -6.2119061, \\ y = 1 \ (p1 \ wants \ to \ see \ p2 \ again ).$

<br>

Our variable's values would render the following formula in terms of standard library output:

<br>

$$\large log(\frac{P(y = 1\ | \ x)}{1 - P(y = 1\ | \ x)}) =-6.2119061+0.9441278 \cdot x_{1}, \; \normalsize where: \ y = dec;\ x_{1}=like;$$

<br>

Knowing that $e^{-6.2119061} \sim  0.002005411$ the more meaningful formula with the actual exponentiation would look like:
<br>

$$\large \frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)} =0.002005411 \cdot e^{b_{1} \cdot x}, \; \normalsize where: \ y = dec;\ x=like;$$

<br>

Which depending on $x$  will look like:

<br>

$\large \frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)} =0.002005411, \; \normalsize if: \ x=0;$

$\large \frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)} =0.002005411 \cdot e^{b_{1}}, \; \normalsize where: \ x=1;$

$\large \frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)} =0.002005411 \cdot e^{2 \cdot b_{1}}, \; \normalsize where: \ x=2;$

<br>

And so forth...

<br>

Notice that at the end how the formula changes depends mostly on the term $\large e^{b_{1} \cdot x}$. If we have the exponentiation $A^{B}$ we have three possibilities:

$B > 0 \ $ : Then $A^{B} > 1$ and $A^{B}$  will be bigger the bigger $B$ is.

$B = 0 \ $ : Then $A^{B} = 1$ 

$B < 0 \ $: Then $A^{B}$ boils down to $\frac{1}{A^{B}}$  which will be a smaller fraction the bigger $B$ is.

<br>

Mind now that our $b1 > 0$ or in other words $e^{b1} > 1$, therefore it will have a positive effect on $\frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)}$, and the bigger it's the stronger the positive effect. Therefore $like$ has a positive effect on the _**oddratio**_ of $dec = 1$ over $dec = 0$.    

<br>

***

<br>

# Analysis Questions

<br>

> Which factors (predictors) have a significant effect on the chance of p1 deciding to meet with p2 again? Are their effect positive or negative?

> Which factor (predictor) has the most effect (relevance) on the chance of p1 deciding to meet with p2 again?

<br>

***

<br>

# Logistic Regression

```{r}
glm(dec ~ like + fun + attr + shar + sinc + prob + amb + intel + intel * shar,
      data = training_data, 
      family = "binomial") -> bm

glance(bm)
```

<br>

***

<br>

## Residual Analysis

<br>

> Residual Analysis can be very hard to use to help you understand what is going on with your logistic regression model. So we should take what they say with a grain of salt and probably give more weight to what $McFadden's \ pseudo \ R2$ said.

<br>

Let's keep the residual data in a specific data frame

```{r}
mod.res <- resid(bm)
std.resid <- rstandard(bm)
dec <- training_data$dec

resid_data <- data.frame(mod.res,std.resid,training_data,
                       stringsAsFactors=FALSE)
resid_data %>% 
  sample_n(10)
```

### Against each predictor

```{r}
resid_data %>% 
  ggplot(aes(intel,mod.res)) + 
  geom_point(alpha=0.4) + 
  labs(x = "Predictor Intelligence (intel)",
       y = "Model Residual") -> intel_resid

resid_data %>% 
  ggplot(aes(fun,mod.res)) + 
  geom_point(alpha=0.4) + 
  labs(x = "Predictor Funny (fun)",
       y = "Model Residual") -> fun_resid

gridExtra::grid.arrange(intel_resid,
                        fun_resid, 
                        ncol = 2)
```

* No alarming results in terms of extreme values, obviously non-linear patterns nor heteroscedasticity.

```{r}
resid_data %>% 
  ggplot(aes(attr,mod.res)) + 
  geom_point(alpha=0.4) +
  labs(x = "Predictor Attractiveness (attr)",
       y = "Model Residual") -> attr_resid

resid_data %>% 
  ggplot(aes(amb,mod.res)) + 
  geom_point(alpha=0.4) +
  labs(x = "Predictor Ambition (amb)",
       y = "Model Residual") -> amb_resid

gridExtra::grid.arrange(attr_resid,
                        amb_resid, 
                        ncol = 2)
```

* No alarming results in terms of extreme values, obviously non-linear patterns nor heteroscedasticity.

```{r}
resid_data %>% 
  ggplot(aes(sinc,mod.res)) + 
  geom_point(alpha=0.4) +
  labs(x = "Predictor Sincerity (sinc)",
       y = "Model Residual") -> sinc_resid

resid_data %>% 
  ggplot(aes(like,mod.res)) + 
  geom_point(alpha=0.4) + 
  labs(x = "Predictor Like (like)",
       y = "Model Residual") -> like_resid

gridExtra::grid.arrange(sinc_resid, 
                        like_resid,
                        ncol=2)
```

* No alarming results in terms of extreme values, obviously non-linear patterns nor heteroscedasticity.

```{r}
resid_data %>% 
  ggplot(aes(shar,mod.res)) + 
  geom_point(alpha=0.4) +
  labs(x = "Predictor Shared (shar)",
       y = "Model Residual") -> shar_resid

resid_data %>% 
  ggplot(aes(prob,mod.res)) + 
  geom_point(alpha=0.4) + 
  labs(x = "Predictor Probability (prob)",
       y = "Model Residual") -> prob_resid

gridExtra::grid.arrange(shar_resid, 
                        prob_resid,
                        ncol=2)
```

* No alarming results in terms of extreme values, obviously non-linear patterns nor heteroscedasticity.

<br>

#### Looking at each predictor against the model residual we can say:

* They‚Äôre not symmetrically distributed, nor they tend to cluster towards the middle of the plot.
* They‚Äôre relatively clustered around the lower single digits of the y-axis (e.g., ideally would be 0.5 or 1.5), while not ideal it could be worse.



### Against whole model

```{r}
bm %>%
  ggplot(aes(.fitted, .resid)) + 
  geom_point() +
  stat_smooth(method="loess") + 
  geom_hline(yintercept=0, col="red", linetype="dashed") + 
  xlab("Fitted values") + ylab("Residuals") + 
  ggtitle("Residual vs Fitted Plot")
```

* There's no distinctive pattern in the plot, therefore this suggests that there weren't non-linear patterns in the data that couldn't be explained by the model and were left out in the residuals.

> The data doesn't seem to demand a non-linear regression.


```{r}
y <- quantile(resid_data$std.resid[!is.na(resid_data$std.resid)], c(0.25, 0.75))
x <- qnorm(c(0.25, 0.75))
slope <- diff(y)/diff(x)
int <- y[1L] - slope * x[1L]

resid_data %>%
  ggplot(aes(sample=std.resid)) +
  stat_qq(shape=1, size=3) +      # open circles
  labs(title="Normal Q-Q",        # plot title
  x="Theoretical Quantiles",      # x-axis label
  y="Standardized Residuals") +   # y-axis label
  geom_abline(slope = slope,
              color = "red",
              size = 0.8,
              intercept = int,
              linetype="dashed")  # dashed reference line
```

* There's some deviation from the normal distribution on both left and right side of the qq plot, but otherwise the standardized residuals are not that far way from the normal distribution. _For a **logistic regression** our qqplot is very well behaved_.
* The Normal Q-Q plot helps you detect if your residuals are normally distributed. But the deviance residuals don't have to be normally distributed for the model to be valid, so the normality / non-normality of the residuals doesn't necessarily tell you anything.

> This suggests that there would be a better combination of predictors to be found, although the current one still seems considerably appropriate.



```{r}
bm %>%
  ggplot(aes(.fitted, 
             sqrt(abs(.stdresid)))) + 
  geom_point(na.rm=TRUE) + 
  stat_smooth(method="loess",
              na.rm = TRUE) +
  labs(title = "Scale-Location",
       x= "Fitted Value",
       y = expression(sqrt("|Standardized residuals|")))
```

* The residuals appear to be somewhat randomly spread, however there's a tendency in them to form a parabola. 
* There's some degree of violation on the assumption of equal variance (homoscedasticity).

> Confirmation of the qqplot results.

```{r}
bm %>%
  ggplot(aes(.hat, .stdresid)) + 
  geom_point(aes(size=.cooksd), na.rm=TRUE) +
  stat_smooth(method="loess", na.rm=TRUE) +
  xlab("Leverage")+ylab("Standardized Residuals") + 
  ggtitle("Residual vs Leverage Plot") + 
  scale_size_continuous("Cook's Distance", range=c(1,5)) +    
  theme(legend.position="bottom")
```

* All the occurrences have low values of Cook's Distance (below 0.1).

> There are no "outliers"/extreme values who are influential cases (i.e., subjects) and would therefore have an undue influence on the regression line.

<br>

```
Overall our residua analysis suggests that our regression fit the data very well. 
There may be a better model out there but ours does a pretty good job. 
```

<br>

## Model Coefficients

```{r}
tidy(bm, conf.int = TRUE)
```

```{r}
broom::tidy(bm,
            conf.int = TRUE,
            conf.level = 0.95) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(term, estimate,
             ymin = conf.low,
             ymax = conf.high)) +
  geom_errorbar(size = 0.8, width= 0.4) +
  geom_point(color = "red", size = 2) +
  geom_hline(yintercept = 0, colour = "darkred") +
  labs(x = "Predictor variable",
       title = "Logistic regression terms",
       y = expression(paste("estimated ", 'b'," (95% confidence)")))
```

The coefficients here follow a generalization of the formula $$\normalsize log(\frac{P(y\ | \ x)}{1 - P(y\ | \ x)}) =b_{0}+b_{1} \cdot x_{1} \;$$

This particular alternative doesn't give a clear idea on the magnitude of the coefficients' effect. There are however some things that can be extracted:

* **intel** and **shar * intel** have no significant effect as (their C.I) regarding $b$ intersect 0.
* **amb, attr, fun, like, prob, shar** and **sinc** have significant effect as (their C.I) don't intersect 1.
    + **amb** and **sinc** have a negative effect on the on the response variable.
    + **attr, fun, like ,prob** and **shar** have a positive effect on the response variable.

<br>

* **like** and **attr** seem to have the highest effect on the response variable, farthest from 0.
    + Despite intersection with **attr**, **like** seems to have the biggest impact on the response variable.
    
<br>

```{r}
# EXPONENTIATING:
tidy(bm, conf.int = TRUE, exponentiate = TRUE)
```

```{r}
broom::tidy(bm,
            conf.int = TRUE,
            conf.level = 0.95,
            exponentiate = TRUE) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(term, estimate,
             ymin = conf.low,
             ymax = conf.high)) +
  geom_errorbar(size = 0.8, width= 0.4) +
  geom_point(color = "red", size = 2) +
  geom_hline(yintercept = 1, colour = "darkred") +
  labs(x = "Predictor variable",
       title = "Exponentiated logistic regression terms",
       y = expression(paste("estimated ", 'e'^{'b'}," (95% confidence)"))) +
  ylim(0,4)
```

The coefficients here follow a generalization of the formula $$\normalsize \frac{P(y\ | \ x)}{1 - P(y \ | \ x)} =e^{b_{0}+b_{1} \cdot x_{1}} \;$$

As mentioned in the previous section when $e^{b} > 1$ its effect is positive and when $e^{b} < 1$ it has a negative effect. Therefore:

* **intel** and **shar * intel** have no significant effect as (their C.I) intersect 1.

* **amb, attr, fun, like, prob, shar** and **sinc** have significant effect as (their C.I) don't intersect 1.
    + **amb** and **sinc** have a negative effect on the **oodratio** of $dec = 1$ over $dec = 0$.
    + **attr, fun, like ,prob** and **shar** have a positive effect on the **oodratio** of $dec = 1$ over $dec = 0$.

<br>

A multiplier of negative effect is strongest _the farther it's from 1 and the closer it's to 0_ while A multiplier of positive effect is strongest _the farther it's from 1 and the closer it's to $+\infty$_.

<br>

##### The predictor **like** is the most relevant of the candidates i.e. it's the one has that the most effect on the chance of p1 deciding to meet with p2 again. 

* The predictor **like**'s estimate range is the farthest from 1. To have an idea C.I for **like** is around 3 so it's basically tripling the **oddratio** when $like = 1$. 
* To have a multiplier of negative effect comparable to **like** its C.I. would need to be around $\frac{1}{3}$, so it would cut by three the **oddratio** when its value is 1 the same way **like** triples it when $like = 1$. 
* As we can see in the plot the range of all other predictors in terms of C.I. are far too close to 1 to compare to **like** with the exception of **attr**.
    + While **attr** intersects **like** the intersection is not big enough to completely dismiss any possibility of significant difference between them. For this reason we'll still consider **like** as the most relevant with **attr** as dangerously close second. 

<br>

***

<br>

# Cross Validation

<br>

## ROC Curve

```{r}
# Compute AUC for predicting Class with the model
prob <- predict(bm, newdata=testing_data, type="response")
pred <- prediction(prob, testing_data$dec)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")

autoplot(perf) +
  labs(x="False Positive Rate", 
       y="True Positive Rate",
       title="ROC Curve")
```

```{r}
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
```
* AUC values above 0.80 indicate that the model does a good job.
* With a result of $0.863979$ the computed AUC for the predicting Class shows that the model does a good job in discriminating between the two categories which comprise our target variable.

<br>

## Classification Rate

```{r}
predictions = bm %>% 
  augment(type.predict = "response") %>% 
  mutate(acc_to_model = .fitted > .5, 
         acc_to_data = dec == "yes")

table(predictions$acc_to_model, predictions$acc_to_data)
```

```{r}
xtabs(~ acc_to_model + acc_to_data, data = predictions)
```

```{r}
mosaic(acc_to_model ~ acc_to_data, data = predictions, 
       shade = T)
```

```{r}
predictions %>%
 summarise(accuracy = sum(acc_to_model == acc_to_data) / n(),
           false_positives = n())
```

* Our model rendered a pretty decent accuracy rate of $0.765925$.

<br>

## McFadden's pseudo R2

```{r}
pR2(bm)
```

* $Rho-squared$ also known as $McFadden's \ pseudo \ R2$ can be interpreted like R2, but its values tend to be considerably lower than those of the R2 index. And values from 0.2-0.4 indicate (in McFadden's words) excellent model fit. Our $0.3221384$ therefore represents an excellent fit in terms of $McFadden's \ pseudo \ R2$.

<br>

***

<br>

# Conclusion

<br>

## Significance

<br>

* **amb, attr, fun, like, prob, shar** and **sinc** have significant effect as (their C.I) don't intersect 1 in terms of $e^{b}$ nor 0 in terms of $b$.
    + **amb** and **sinc** have a negative effect on the **oodratio** of $dec = 1$ over $dec = 0$.
    + **attr, fun, like ,prob** and **shar** have a positive effect on the **oddratio** of $dec = 1$ over $dec = 0$.

<br>

## Most relevant predictor

<br>

* The predictor **like** is the most relevant of the candidates i.e. it's the one has that the most effect on the chance of p1 deciding to meet with p2 again with **attr** close as second. 
    + Despite intersection we opted to choose **like** as winner for we judged their intersection was not big enough to dismiss the possibility of difference between them.
