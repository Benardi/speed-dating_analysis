---
title: "Logistic Regression on speed dating data"
author: "Jos√© Benardi de Souza Nunes"
date: 05/08/2018
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---

<br>

> This report contains regression models created based on data describing 5000 speed dates of 4 minutes of duration involving 310 american young adults. The original data were collected by Columbia Business professors. Further information and the data itself can be found in this [report repository](https://github.com/Benardi/speed-dating_analysis).

<br>

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(broom)
library(pscl)
library(here)

theme_set(theme_bw())
```

# Data Overview

<br>

## The variables
<br>

```
The response variable is the variable that you are interested in learn something about.

A predictor variable is a variable used in regression to predict another variable.

Our response variable will be "dec", we want to study how well the predictor variables can help predict its behavior and how they impact it.
```

<br>

##### Each speed date had two participants, **p1** (participant 1) and **p2** (participant 2). For each speed date we the following variable were collected:

<br>

- **iid** : id of the participant p1 in the date
- **gender** : gender of p1, 0 = woman
- **order** : of the several dates in the night, this was the nth, according to this variable
- **pid** : id of participant p2
- **int_corr** : correlation between the interests of p1 and p2
- **samerace** : Are p1 and p2 of the same race?
- **age_o** : Age of p2
- **age** : Age of p1
- **field** : field of study of p1
- **race** : race of p1. The code is Black/African American=1; European/Caucasian-American=2; Latino/Hispanic American=3; Asian/Pacific Islander/Asian-American=4; Native American=5; Other=6
- **from** : from where p1 comes from
- **career** : what career p1 wants to follow sports, tvsports, exercise, dining, museums, art, hiking, gaming, clubbing, reading, tv, theater, movies, concerts, music, shopping, yoga : From 1 to 10, how interested p1 is in each one of these activities$
- **attr** : how attractive p1 thinks p2 is
- **sinc** : how sincere p1 thinks p2 is
- **intel** : how smart p1 thinks p2 is
- **fun** : how fun p1 thinks p2 is
- **amb** : how ambitious p1 thinks p2 is
- **shar** : how much p1 believes they both (p1 and p2) share the same interests and hobbies
- **like** : in general, how much does p1 likes p2?
- **prob** : how probable p1 thinks it's that p2 will want to meet again with p- (scale 1-10)
- **attr3_s** : how attractive p1 believes itself
- **sinc3_s** : how sincere p1 believes itself
- **intel3_s** : how smart p1 believes itself
- **fun3_s** : how fun p1 believes itself
- **amb3_s** : how ambitious p1 believes itself
- **dec** : whether p1 wants to meet p2 again given how the speed date went.

<br>

```{r}
data <- read_csv(here::here("data/speed-dating2.csv"),
                 col_types = cols(
                          .default = col_integer(),
                          int_corr = col_double(),
                          field = col_character(),
                          from = col_character(),
                          career = col_character(),
                          attr = col_double(),
                          sinc = col_double(),
                          intel = col_double(),
                          fun = col_double(),
                          amb = col_double(),
                          shar = col_double(),
                          like = col_double(),
                          prob = col_double(),
                          match_es = col_double(),
                          attr3_s = col_double(),
                          sinc3_s = col_double(),
                          intel3_s = col_double(),
                          fun3_s = col_double(),
                          amb3_s = col_double(),
                          dec = col_character()
                        )) %>% 
  mutate(dec = factor(dec),
         gender = factor(gender),
         samerace = factor(samerace),
         race = factor(race))

data %>%
  glimpse()
```

# Explanation on logistic regression

<br>

> If you already know your way around logistic regression feel free to skip this section. If you ever feel unsure about it feel free to come back here and consult it.

<br>

## Formulas and definitions

<br>

Let's use as example our aforementioned response variable $dec$, and let's suppose we'll use as our sole predictor $like$. In the end it all boils down to a  conditional probability for a certain value of $dec$ and $like$:

<br>

$$\large P(y \ | \ x ) = z, \normalsize where: \ y = dec;\ x=like;$$

<br>

As you may have noticed $dec$ is a binary variable (p1 either says **yes** or **no**), for this reason we work with a sigmoid function for the probability of a certain outcome of $dec$:  

<br>

$$\large P(y\ | \ x)=\frac{e^{b_{0}+b_{1} \cdot x}}{1 + e^{b_{0}+b_{1} \cdot x}}, \; \normalsize where: \ y = dec;\ x=like;$$

<br>

However, to talk about how the predictor $like$ impacts the response variable $dec$, which means talking about $b_{1}$ it's more convenient to talk in terms of _**odds ratio**_:

<br>

$$\large \frac{P(y\ | \ x)}{1 - P(y \ | \ x)} =e^{b_{0}+b_{1} \cdot x_{1}}, \; \normalsize where: \ y = dec;\ x_{1}=like;$$

<br>

The libraries usually render the coefficients in the following form:

<br>

$$\large log(\frac{P(y\ | \ x)}{1 - P(y\ | \ x)}) =b_{0}+b_{1} \cdot x_{1}, \; \normalsize where: \ y = dec;\ x_{1}=like;$$


<br>

## Numeric example

<br>

In my experience an example with actual numbers helps a lot, so let's assume these are the following values for our variables (by means of the logistic regression):

 $b1 = 0.9441278, \\ b0 = -6.2119061, \\ y = 1 \ (p1 \ wants \ to \ see \ p2 \ again ).$

<br>

Our variable's values would render the following formula in terms of standard librarie output:

<br>

$$\large log(\frac{P(y = 1\ | \ x)}{1 - P(y = 1\ | \ x)}) =-6.2119061+0.9441278 \cdot x_{1}, \; \normalsize where: \ y = dec;\ x_{1}=like;$$

<br>

Knowing that $e^{-6.2119061} \sim  0.002005411$ the more meaningful formula with the actual exponentiations would look like:
<br>

$$\large \frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)} =0.002005411 \cdot e^{b_{1} \cdot x}, \; \normalsize where: \ y = dec;\ x=like;$$

<br>

Which depending on $x$  will look like:

<br>

$\large \frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)} =0.002005411, \; \normalsize if: \ x=0;$

$\large \frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)} =0.002005411 \cdot e^{b_{1}}, \; \normalsize where: \ x=1;$

$\large \frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)} =0.002005411 \cdot e^{2 \cdot b_{1}}, \; \normalsize where: \ x=2;$

<br>

And so forth...

<br>

Notice that at the end how the formula changes depends mostly on the term $\large e^{b_{1} \cdot x}$. If we have the exponentiation $A^{B}$ we have three possibilites:

$B > 0 \ $ : Then $A^{B} > 1$ and $A^{B}$  will be bigger the bigger $B$ is.

$B = 0 \ $ : Then $A^{B} = 1$ 

$B < 0 \ $: Then $A^{B}$ boils down to $\frac{1}{A^{B}}$  which will be a smaller fraction the bigger $B$ is.

<br>

Mind now that our $b1 > 0$ or in other words $e^{b1} > 1$, therefore it will have a positive effect on $\frac{P(y=1 \ | \ x)}{1 - P(y=1 \ | \ x)}$, and the bigger it's the stronger the positive effect. Therefore $like$ has a positive effect on the _**oddratio**_ of $dec = 1$ over $dec = 0$.    

<br>

***

<br>

# Model with candidate predictors

```{r}
data %>%
  na.omit(dec, like, fun, attr, shar, sinc, prob, amb, intel) -> data
  
glm(dec ~ like + fun + attr + shar + sinc + prob + amb + intel + intel * shar,
      data = data, 
      family = "binomial") -> bm

tidy(bm, conf.int = TRUE) %>% 
  select(-statistic, -p.value)
```

```{r}
# EXPONENTIATING:
tidy(bm, conf.int = TRUE, exponentiate = TRUE) %>%
  select(-statistic, -p.value)
```

```{r}
glance(bm)
```

```{r}
pR2(bm)
```

***

<br>

# Conclusion

<br>

##### Our two questions are:

<br>

> Which factors (predictors) have a significant effect on the chance of p1 deciding to meet with p2 again? Are their effect positive or negative?

> Which factor (predictor) has the most effect (relevance) on the chance of p1 deciding to meet with p2 again?

```{r}
broom::tidy(bm,
            conf.int = TRUE,
            conf.level = 0.95,
            exponentiate = TRUE) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(term, estimate,
             ymin = conf.low,
             ymax = conf.high)) +
  geom_errorbar(size = 0.8, width= 0.4) +
  geom_point(color = "red", size = 2) +
  geom_hline(yintercept = 1, colour = "darkred") +
  labs(x = "Predictor variable",
       title = "Exponentiated logistic regression terms",
       y = expression(paste("estimated ", 'e'^{'B'}," (95% confidence)"))) +
  ylim(0,3)
```

## Significance

<br>

As mentioned in the previous section when $e^{b1} > 1$ its effect is positive and when $e^{b1} < 1$ it has a negative effect. Therefore:

* **fun, intel, shar** and **shar * intel** have no significant effect as (their C.I) intersect 1.

* **amb,  attr, like, prob** and **sinc** have significant effect as (their C.I) don't intersect 1.
    + **amb** and **sinc** have a negative effect on the **oodratio** of $dec = 1$ over $dec = 0$.
    + **attr, like** and **prob** have a positive effect on the **oodratio** of $dec = 1$ over $dec = 0$.

<br>

## Most relevant predictor

<br>

A multiplier of negative effect is strongest _the farther it's from 1 and the closer it's to 0_ while A multiplier of positive effect is strongest _the farther it's from 1 and the closer it's to $+\infty$_.

<br>

##### The predictor **like** is the most relevant of the candidates i.e. it's the one has that the most effect on the chance of p1 deciding to meet with p2 again. 

* The predictor **like**'s estimate range is the farthest from 1. To have an idea C.I for **like** is around 2 so it's basically  doubling the **oddratio** when $like = 1$. 
* To have a multiplier of negative effect comparable to **like** its C.I. would need to be around 0.5, so it would cut by half the **oddratio** when its value iis 1 the same way **like** doubles it when $like = 1$. 
* As we can see in the plot the range of all other predictors (**fun, attr, shar, sinc, prob, amb, intel, intel \cdot shar**) in terms of C.I. are far too close to 1 to compare to **like**.

