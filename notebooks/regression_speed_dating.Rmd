---
title: "Regression on speed dating data"
author: "Jos√© Benardi de Souza Nunes"
date: 28/07/2018
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
---

<br>

> This report contain regression models created based on data describing  5000 speed dates of 4 minutes of duration involving 310 american young adults. The original data were collected by Columbia Business professors. Further information and the data itself can be found in this [report repository](https://github.com/Benardi/speed-dating_analysis).

<br>

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(modelr)
library(GGally)
library(caret)
library(broom)
library(here)

theme_set(theme_bw())
```

# Data Overview

```{r}
data <- read_csv(here("data/speed-dating.csv"),
                 progress = FALSE,
                 col_types =cols(.default = col_integer(),
                                 int_corr = col_double(),
                                 field = col_character(),
                                 from = col_character(),
                                 career = col_character(),
                                 attr = col_double(),
                                 samerace = col_character(),
                                 sinc = col_double(),
                                 intel = col_double(),
                                 fun = col_double(),
                                 amb = col_double(),
                                 shar = col_double(),
                                 like = col_double(),
                                 prob = col_double(),
                                 match_es = col_double(),
                                 attr3_s = col_double(),
                                 sinc3_s = col_double(),
                                 intel3_s = col_double(),
                                 fun3_s = col_double(),
                                 amb3_s = col_double())) %>%
  mutate(from = as.numeric(factor(from)),
         gender = as.numeric(factor(gender)),
         samerace = as.numeric(factor(samerace)))

data %>%
  glimpse()
```

<br>

## The variables

<br>

- **iid** : id of the participant p1 in the date
- **gender** : gender of p1, 0 = woman
- **order** : of the several dates in the night, this was the nth, according to this variable
- **pid** : id of participant p2
- **int_corr** : correlation between the interests of p1 and p2
- **samerace** : Are p1 and p2 of the same race?
- **age_o** : Age of p2
- **age** : Age of p1
- **field** : field of study of p1
- **race** : race of p1. The code is Black/African American=1; European/Caucasian-American=2; Latino/Hispanic American=3; Asian/Pacific Islander/Asian-American=4; Native American=5; Other=6
- **from** : from where p1 comes from
- **career** : what career p1 wants to follow sports, tvsports, exercise, dining, museums, art, hiking, gaming, clubbing, reading, tv, theater, movies, concerts, music, shopping, yoga : From 1 to 10, how interested p1 is in each one of these activities$
- **attr** : how attractive p1 thinks p2 is
- **sinc** : how sincere  p1 thinks p2 is
- **intel** : how smart p1 thinks p2 is
- **fun** : how fun p1 thinks p2 is
- **amb** : how ambitious p1 thinks p2 is
- **shar** : how much p1 believes they both (p1 and p2) share the same interests and hobbies
- **like** : in general, how much does p1 likes p2?
- **prob** : how probable p1 thinks it's that p2 will want to meet again with p- (scale 1-10)
- **attr3_s** : how attractive p1 believes itself
- **sinc3_s** : how sincere p1 believes itself
- **intel3_s** : how smart p1 believes itself
- **fun3_s** : how fun p1 believes itself
- **amb3_s** : how ambitious p1 believes itself

<br>

```{r}
data %>%
  na.omit(race) %>%
  ggplot(aes(race, ..prop..)) +
  geom_bar()
```

* Most of the participants are white (code = 2)
* There were no native americans involved (code = 5) 

## Choosing promising candidate variables

```{r}
require(GGally)

data %>% 
  select(like,fun,amb,attr,
         sinc,intel,shar,prob,
         fun3_s,amb3_s,attr3_s,
         sinc3_s,intel3_s,samerace,
         gender,from) %>% 
  na.omit() %>%
  ggcorr(palette = "RdBu", label = TRUE,
       hjust = 0.75, label_size = 3, nbreaks = 5) +
  ggtitle("Correlation plot for employed variables")
```

* What a person (p1) believes about herself/himself doesn't show promising results in terms of correlation
* Among what p1 thinks about p2 how *ambitious* p1 thinks p2 is shows the weakest correlation with how much p1 likes p2.
* Intelligence has interesting interactions with sincerity and ambition.

<br>

```{r}
data %>% 
  select(like,fun,attr,sinc,
         intel,shar,prob) %>% 
  na.omit() %>%
  ggpairs(upper = list(continuous = "density"), 
        lower = list(continuous = wrap("cor", size=5)),
        axisLabels = 'show',progress = F) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Regarding the relatinship with the response variable "**like**":

* Looking at the bidimensional density plots the variables **fun** and **attr** have a cleaner and clearer connection with **like** as expected from what we saw in terms of correlation.
* Despite unpromising results in correlation **prob** has a somewhat clean interaction with **like** in terms of bidimensional density plot. Might be worth looking into.

<br>

```{r}
data %>%
  na.omit(fun, like) %>%
  ggplot(aes(fun, like)) +
  stat_density2d(aes(fill = ..level..),
                 geom = "polygon")
```

* Intuition would suggest a positive interaction between being seen as fun and being liked, that interaction being of considerable magnitude.

```{r}
data %>%
  na.omit(attr, like) %>%
  ggplot(aes(attr, like)) +
  stat_density2d(aes(fill = ..level..),
                 geom = "polygon")
```

* Intuition would suggest a positive interaction between being seen as attractive and being liked (no surprise there), that interaction being of considerable magnitude.

## Splitting Data for Cross Validation

```{r}
data %>%  # Keep only candidate and response variables
  select(fun, prob, order,
         attr, sinc, prob, shar, 
         intel, like, gender, samerace) %>%
  na.omit() -> data  # remove NAs

data %>% ## Put numeric predictor variables on same scale 
   mutate_at(.vars = vars(fun, prob, order,attr,
                          sinc, prob, shar,intel),
             .funs = funs(as.numeric(scale(.)))) -> data_scaled

data_scaled %>%
  glimpse()
```

* Selecting candidate predictors, filtering invalid numbers and putting the variables on appropriate scale

<br>

#### For the sake of simplicity we'll follow the (80/20) thumb rule (based on Pareto's principle) and put 80% of our dataset in the training set and 20% in the test set. 

<br>

```{r}
set.seed(101) # We set the set for reason of reproducibility

## Adding surrogate key to dataframe
data_scaled$id <- 1:nrow(data_scaled)

data_scaled %>% 
  dplyr::sample_frac(.8) -> training

training %>% 
  glimpse()
```

* Randomly selecting the **training data**


```{r}
dplyr::anti_join(data_scaled, 
                 training, 
                 by = 'id') -> testing
testing %>% 
  glimpse()
```

* The rest of the data will be the **testing data** (Disjoint sets)

<br>


***

<br>

# Applying Regression Models

<br>

## Intelligence related Model

```{r}
mod <- lm(like ~ fun + attr + shar + sinc + prob + sinc * intel + intel, 
          data = training)

glance(mod) 
``` 

```{r}
tidy(mod, 
     conf.int = TRUE, 
     conf.level = 0.95)
```

```{r}
tidy(mod, 
     conf.int = TRUE, 
     conf.level = 0.95)  %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(term, estimate, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar(size = 0.8, width= 0.4) +
  geom_point(color = "red", size = 2) +
  geom_hline(yintercept = 0, colour = "darkred")
```

### Residual Analysis

Let's keep the residue data in a specific dataframe

```{r}
mod.res <- resid(mod)
std.resid <- rstandard(mod)
like <- training$like

resid_data <- data.frame(mod.res,std.resid,like,
                       stringsAsFactors=FALSE)
resid_data %>% 
  sample_n(10)
```


```{r}
resid_data %>%
  ggplot(aes(like, mod.res)) +
  geom_point(alpha = 0.4) +
  geom_hline(yintercept = 0,
             color = "red") +
  labs(x = "Response Variable (like)", y = "Residue") +
  ggtitle("Residual Plot")
```

```{r}
mod %>%
  ggplot(aes(.fitted, .resid)) + 
  geom_point() +
  stat_smooth(method="loess") + 
  geom_hline(yintercept=0, col="red", linetype="dashed") + 
  xlab("Fitted values") + ylab("Residuals") + 
  ggtitle("Residual vs Fitted Plot")
```


```{r}
y <- quantile(resid_data$std.resid[!is.na(resid_data$std.resid)], c(0.25, 0.75))
x <- qnorm(c(0.25, 0.75))
slope <- diff(y)/diff(x)
int <- y[1L] - slope * x[1L]

resid_data %>%
  ggplot(aes(sample=std.resid)) +
  stat_qq(shape=1, size=3) +      # open circles
  labs(title="Normal Q-Q",        # plot title
  x="Theoretical Quantiles",      # x-axis label
  y="Standardized Residuals") +   # y-axis label
  geom_abline(slope = slope,
              color = "red",
              size = 0.8,
              intercept = int,
              linetype="dashed")  # dashed reference line
```

```{r}
mod %>%
  ggplot(aes(.fitted, 
             sqrt(abs(.stdresid)))) + 
  geom_point(na.rm=TRUE) + 
  stat_smooth(method="loess",
              na.rm = TRUE) +
  labs(title = "Scale-Location",
       x= "Fitted Value",
       y = expression(sqrt("|Standardized residuals|")))
```

```{r}
mod %>%
  ggplot(aes(.hat, .stdresid)) + 
  geom_point(aes(size=.cooksd), na.rm=TRUE) +
  stat_smooth(method="loess", na.rm=TRUE) +
  xlab("Leverage")+ylab("Standardized Residuals") + 
  ggtitle("Residual vs Leverage Plot") + 
  scale_size_continuous("Cook's Distance", range=c(1,5)) +    
  theme(legend.position="bottom")
```

```{r}
mod %>%
  ggplot(aes(.hat, .cooksd)) + 
  geom_point(na.rm=TRUE) + 
  stat_smooth(method="loess", na.rm=TRUE) + 
  xlab("Leverage hii")+ylab("Cook's Distance") + 
  ggtitle("Cook's dist vs Leverage hii/(1-hii)") + 
  geom_abline(slope=seq(0,3,0.5), color="gray", linetype="dashed")
```

### Cross Validation

<br>

#### Validation Set Approach

```{r}
predictions <- mod %>% predict(testing)

data.frame( R2 = caret::R2(predictions, testing$like),
            RMSE = caret::RMSE(predictions, testing$like),
            MAE = caret::MAE(predictions, testing$like),
            ERR = caret::RMSE(predictions, testing$like)/
              mean(testing$like))
```

## Gender related Model

```{r}
mod <- lm(like ~ fun + attr + gender + shar * gender + sinc + prob, 
          data = training)

glance(mod) 
``` 

```{r}
tidy(mod, 
     conf.int = TRUE, 
     conf.level = 0.95)
```

```{r}
tidy(mod, 
     conf.int = TRUE, 
     conf.level = 0.95)  %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(term, estimate, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar(size = 0.8, width= 0.4) +
  geom_point(color = "red", size = 2) +
  geom_hline(yintercept = 0, colour = "darkred")
```

### Residual Analysis

Let's keep the residue data in a specific dataframe

```{r}
mod.res <- resid(mod)
std.resid <- rstandard(mod)
like <- training$like

resid_data <- data.frame(mod.res,std.resid,like,
                       stringsAsFactors=FALSE)
resid_data %>% 
  sample_n(10)
```


```{r}
resid_data %>%
  ggplot(aes(like, mod.res)) +
  geom_point(alpha = 0.4) +
  geom_hline(yintercept = 0,
             color = "red") +
  labs(x = "Response Variable (like)", y = "Residue") +
  ggtitle("Residual Plot")
```

```{r}
mod %>%
  ggplot(aes(.fitted, .resid)) + 
  geom_point() +
  stat_smooth(method="loess") + 
  geom_hline(yintercept=0, col="red", linetype="dashed") + 
  xlab("Fitted values") + ylab("Residuals") + 
  ggtitle("Residual vs Fitted Plot")
```


```{r}
y <- quantile(resid_data$std.resid[!is.na(resid_data$std.resid)], c(0.25, 0.75))
x <- qnorm(c(0.25, 0.75))
slope <- diff(y)/diff(x)
int <- y[1L] - slope * x[1L]

resid_data %>%
  ggplot(aes(sample=std.resid)) +
  stat_qq(shape=1, size=3) +      # open circles
  labs(title="Normal Q-Q",        # plot title
  x="Theoretical Quantiles",      # x-axis label
  y="Standardized Residuals") +   # y-axis label
  geom_abline(slope = slope,
              color = "red",
              size = 0.8,
              intercept = int,
              linetype="dashed")  # dashed reference line
```

```{r}
mod %>%
  ggplot(aes(.fitted, 
             sqrt(abs(.stdresid)))) + 
  geom_point(na.rm=TRUE) + 
  stat_smooth(method="loess",
              na.rm = TRUE) +
  labs(title = "Scale-Location",
       x= "Fitted Value",
       y = expression(sqrt("|Standardized residuals|")))
```

```{r}
mod %>%
  ggplot(aes(.hat, .stdresid)) + 
  geom_point(aes(size=.cooksd), na.rm=TRUE) +
  stat_smooth(method="loess", na.rm=TRUE) +
  xlab("Leverage")+ylab("Standardized Residuals") + 
  ggtitle("Residual vs Leverage Plot") + 
  scale_size_continuous("Cook's Distance", range=c(1,5)) +    
  theme(legend.position="bottom")
```

```{r}
mod %>%
  ggplot(aes(.hat, .cooksd)) + 
  geom_point(na.rm=TRUE) + 
  stat_smooth(method="loess", na.rm=TRUE) + 
  xlab("Leverage hii")+ylab("Cook's Distance") + 
  ggtitle("Cook's dist vs Leverage hii/(1-hii)") + 
  geom_abline(slope=seq(0,3,0.5), color="gray", linetype="dashed")
```

### Cross Validation

<br>

#### Validation Set Approach

```{r}
predictions <- mod %>% predict(testing)

data.frame( R2 = caret::R2(predictions, testing$like),
            RMSE = caret::RMSE(predictions, testing$like),
            MAE = caret::MAE(predictions, testing$like),
            ERR = caret::RMSE(predictions, testing$like)/
              mean(testing$like))
```


